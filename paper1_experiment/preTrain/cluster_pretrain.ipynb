{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check gpu device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi_id</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3fd66200f964a52000e71ee3</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3fd66200f964a52001e81ee3</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3fd66200f964a52003e51ee3</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3fd66200f964a52003e71ee3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3fd66200f964a52004e41ee3</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     poi_id  clusters\n",
       "0  3fd66200f964a52000e71ee3        53\n",
       "1  3fd66200f964a52001e81ee3       123\n",
       "2  3fd66200f964a52003e51ee3        66\n",
       "3  3fd66200f964a52003e71ee3        26\n",
       "4  3fd66200f964a52004e41ee3        37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cluster\n",
    "df_cluster = pd.read_csv(\"poi_cluster.csv\")\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9989, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>poi_category_id</th>\n",
       "      <th>poi_category_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>UTC_time</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4abc1f51f964a520798620e3</td>\n",
       "      <td>4bf58dd8d48988d1ce941735</td>\n",
       "      <td>Seafood Restaurant</td>\n",
       "      <td>40.781558</td>\n",
       "      <td>-73.975792</td>\n",
       "      <td>-240</td>\n",
       "      <td>Wed Apr 04 23:31:31 +0000 2012</td>\n",
       "      <td>2012-04-04 23:31:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4d4ac10da0ef54814b6ffff6</td>\n",
       "      <td>4bf58dd8d48988d157941735</td>\n",
       "      <td>American Restaurant</td>\n",
       "      <td>40.784018</td>\n",
       "      <td>-73.974524</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sat Apr 07 17:42:24 +0000 2012</td>\n",
       "      <td>2012-04-07 17:42:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4db44994cda1c57c82583709</td>\n",
       "      <td>4bf58dd8d48988d1f1931735</td>\n",
       "      <td>General Entertainment</td>\n",
       "      <td>40.739398</td>\n",
       "      <td>-73.993210</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sun Apr 08 18:20:29 +0000 2012</td>\n",
       "      <td>2012-04-08 18:20:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4a541923f964a52008b31fe3</td>\n",
       "      <td>4bf58dd8d48988d14e941735</td>\n",
       "      <td>American Restaurant</td>\n",
       "      <td>40.785677</td>\n",
       "      <td>-73.976498</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sun Apr 08 20:02:10 +0000 2012</td>\n",
       "      <td>2012-04-08 20:02:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>40f1d480f964a5205b0a1fe3</td>\n",
       "      <td>4bf58dd8d48988d143941735</td>\n",
       "      <td>Breakfast Spot</td>\n",
       "      <td>40.719929</td>\n",
       "      <td>-74.008532</td>\n",
       "      <td>-240</td>\n",
       "      <td>Mon Apr 09 16:20:52 +0000 2012</td>\n",
       "      <td>2012-04-09 16:20:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                    poi_id           poi_category_id  \\\n",
       "0        1  4abc1f51f964a520798620e3  4bf58dd8d48988d1ce941735   \n",
       "1        1  4d4ac10da0ef54814b6ffff6  4bf58dd8d48988d157941735   \n",
       "2        1  4db44994cda1c57c82583709  4bf58dd8d48988d1f1931735   \n",
       "3        1  4a541923f964a52008b31fe3  4bf58dd8d48988d14e941735   \n",
       "4        1  40f1d480f964a5205b0a1fe3  4bf58dd8d48988d143941735   \n",
       "\n",
       "       poi_category_name   latitude  longitude  time_offset  \\\n",
       "0     Seafood Restaurant  40.781558 -73.975792         -240   \n",
       "1    American Restaurant  40.784018 -73.974524         -240   \n",
       "2  General Entertainment  40.739398 -73.993210         -240   \n",
       "3    American Restaurant  40.785677 -73.976498         -240   \n",
       "4         Breakfast Spot  40.719929 -74.008532         -240   \n",
       "\n",
       "                         UTC_time             datetime  \n",
       "0  Wed Apr 04 23:31:31 +0000 2012  2012-04-04 23:31:31  \n",
       "1  Sat Apr 07 17:42:24 +0000 2012  2012-04-07 17:42:24  \n",
       "2  Sun Apr 08 18:20:29 +0000 2012  2012-04-08 18:20:29  \n",
       "3  Sun Apr 08 20:02:10 +0000 2012  2012-04-08 20:02:10  \n",
       "4  Mon Apr 09 16:20:52 +0000 2012  2012-04-09 16:20:52  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load poi sequential data\n",
    "dir = 'E:\\\\Sebnewrepo/Rec_sys_lab/paper1_experiment/'\n",
    "checkin_file = 'ny_ordered.csv'\n",
    "df = pd.read_csv(dir + checkin_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users with < 5 interactoins are removed\n",
      "POIs with < 5 interactoins are removed\n",
      "num of users:1083, num of POIs:9989\n"
     ]
    }
   ],
   "source": [
    "# remove infrequent items and users\n",
    "from copy import deepcopy\n",
    "def rm_infrequent_items(data, min_counts):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['poi_id'].value_counts()\n",
    "    df = df[df['poi_id'].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"POIs with < {} interactoins are removed\".format(min_counts))\n",
    "    return df\n",
    "def rm_infrequent_users(data, min_counts):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['user_id'].value_counts()\n",
    "    df = df[df[\"user_id\"].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"users with < {} interactoins are removed\".format(min_counts))\n",
    "    return df\n",
    "          \n",
    "filtered_df = rm_infrequent_users(df, 5)\n",
    "filtered_df = rm_infrequent_items(filtered_df, 5)\n",
    "print('num of users:{}, num of POIs:{}'.format(len(filtered_df['user_id'].unique()), len(filtered_df['poi_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>poi_category_id</th>\n",
       "      <th>poi_category_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>UTC_time</th>\n",
       "      <th>datetime</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4abc1f51f964a520798620e3</td>\n",
       "      <td>4bf58dd8d48988d1ce941735</td>\n",
       "      <td>Seafood Restaurant</td>\n",
       "      <td>40.781558</td>\n",
       "      <td>-73.975792</td>\n",
       "      <td>-240</td>\n",
       "      <td>Wed Apr 04 23:31:31 +0000 2012</td>\n",
       "      <td>2012-04-04 23:31:31</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4d4ac10da0ef54814b6ffff6</td>\n",
       "      <td>4bf58dd8d48988d157941735</td>\n",
       "      <td>American Restaurant</td>\n",
       "      <td>40.784018</td>\n",
       "      <td>-73.974524</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sat Apr 07 17:42:24 +0000 2012</td>\n",
       "      <td>2012-04-07 17:42:24</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4db44994cda1c57c82583709</td>\n",
       "      <td>4bf58dd8d48988d1f1931735</td>\n",
       "      <td>General Entertainment</td>\n",
       "      <td>40.739398</td>\n",
       "      <td>-73.993210</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sun Apr 08 18:20:29 +0000 2012</td>\n",
       "      <td>2012-04-08 18:20:29</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4a541923f964a52008b31fe3</td>\n",
       "      <td>4bf58dd8d48988d14e941735</td>\n",
       "      <td>American Restaurant</td>\n",
       "      <td>40.785677</td>\n",
       "      <td>-73.976498</td>\n",
       "      <td>-240</td>\n",
       "      <td>Sun Apr 08 20:02:10 +0000 2012</td>\n",
       "      <td>2012-04-08 20:02:10</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>40f1d480f964a5205b0a1fe3</td>\n",
       "      <td>4bf58dd8d48988d143941735</td>\n",
       "      <td>Breakfast Spot</td>\n",
       "      <td>40.719929</td>\n",
       "      <td>-74.008532</td>\n",
       "      <td>-240</td>\n",
       "      <td>Mon Apr 09 16:20:52 +0000 2012</td>\n",
       "      <td>2012-04-09 16:20:52</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                    poi_id           poi_category_id  \\\n",
       "0        1  4abc1f51f964a520798620e3  4bf58dd8d48988d1ce941735   \n",
       "1        1  4d4ac10da0ef54814b6ffff6  4bf58dd8d48988d157941735   \n",
       "2        1  4db44994cda1c57c82583709  4bf58dd8d48988d1f1931735   \n",
       "3        1  4a541923f964a52008b31fe3  4bf58dd8d48988d14e941735   \n",
       "4        1  40f1d480f964a5205b0a1fe3  4bf58dd8d48988d143941735   \n",
       "\n",
       "       poi_category_name   latitude  longitude  time_offset  \\\n",
       "0     Seafood Restaurant  40.781558 -73.975792         -240   \n",
       "1    American Restaurant  40.784018 -73.974524         -240   \n",
       "2  General Entertainment  40.739398 -73.993210         -240   \n",
       "3    American Restaurant  40.785677 -73.976498         -240   \n",
       "4         Breakfast Spot  40.719929 -74.008532         -240   \n",
       "\n",
       "                         UTC_time             datetime  clusters  \n",
       "0  Wed Apr 04 23:31:31 +0000 2012  2012-04-04 23:31:31       115  \n",
       "1  Sat Apr 07 17:42:24 +0000 2012  2012-04-07 17:42:24       115  \n",
       "2  Sun Apr 08 18:20:29 +0000 2012  2012-04-08 18:20:29       143  \n",
       "3  Sun Apr 08 20:02:10 +0000 2012  2012-04-08 20:02:10       109  \n",
       "4  Mon Apr 09 16:20:52 +0000 2012  2012-04-09 16:20:52       151  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filtered_df.merge(df_cluster, how = 'left', on = 'poi_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  cluster_id\n",
       "0        1         114\n",
       "1        1         114\n",
       "2        1         142\n",
       "3        1         108\n",
       "4        1         150"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = pd.DataFrame({\n",
    "    'user_id': df['user_id'],  # user_id offset by 1\n",
    "    'cluster_id': df['clusters'] - 1,\n",
    "    #'implicit': np.ones(179468)\n",
    "})\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input['cluster_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy \n",
    "def convert_data(data):\n",
    "    df = deepcopy(data)\n",
    "    data = df.groupby('user_id')['cluster_id'].apply(list)\n",
    "    unique_data = df.groupby('user_id')['cluster_id'].nunique()\n",
    "    print(data[:10])\n",
    "    print(len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "1     [114, 114, 142, 108, 150, 171, 59, 23, 59, 197...\n",
      "2     [102, 51, 40, 99, 3, 172, 81, 81, 65, 81, 65, ...\n",
      "3     [131, 0, 61, 172, 43, 29, 180, 114, 0, 52, 61,...\n",
      "4     [124, 124, 124, 124, 124, 0, 124, 52, 33, 110,...\n",
      "5     [168, 168, 0, 50, 79, 50, 0, 50, 51, 51, 51, 1...\n",
      "6     [44, 14, 44, 164, 82, 120, 44, 13, 4, 82, 44, ...\n",
      "7     [0, 42, 42, 33, 114, 0, 81, 69, 8, 83, 0, 79, ...\n",
      "8     [81, 81, 81, 37, 81, 81, 81, 81, 0, 61, 81, 81...\n",
      "9     [39, 183, 81, 108, 39, 133, 10, 100, 141, 51, ...\n",
      "10    [36, 0, 0, 170, 131, 0, 65, 65, 6, 20, 48, 81,...\n",
      "Name: cluster_id, dtype: object\n",
      "1083\n"
     ]
    }
   ],
   "source": [
    "seq_data = convert_data(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Encoder Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class att_encoder(nn.Module):\n",
    "    def __init__(self, num_users, num_items, model_args, device):\n",
    "        super(att_encoder, self).__init__()\n",
    "\n",
    "        self.args = model_args\n",
    "\n",
    "        # init args\n",
    "        L = self.args.L\n",
    "        dims = self.args.d\n",
    "\n",
    "        # user and item embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_users, dims).to(device)\n",
    "        self.item_embeddings = nn.Embedding(num_items, dims).to(device)\n",
    "\n",
    "        self.att_weight_item = Variable(torch.zeros(dims, 1).type(torch.FloatTensor), requires_grad=True).to(device)\n",
    "        self.att_weight_user = Variable(torch.zeros(dims, L).type(torch.FloatTensor), requires_grad=True).to(device)\n",
    "        self.att_weight_item = torch.nn.init.xavier_uniform_(self.att_weight_item)\n",
    "        self.att_weight_user = torch.nn.init.xavier_uniform_(self.att_weight_user)\n",
    "\n",
    "        self.W2 = nn.Embedding(num_items, dims, padding_idx=0).to(device)\n",
    "        self.b2 = nn.Embedding(num_items, 1, padding_idx=0).to(device)\n",
    "\n",
    "        # weight initialization\n",
    "        self.user_embeddings.weight.data.normal_(0, 1.0 / self.user_embeddings.embedding_dim)\n",
    "        self.item_embeddings.weight.data.normal_(0, 1.0 / self.item_embeddings.embedding_dim)\n",
    "        self.W2.weight.data.normal_(0, 1.0 / self.W2.embedding_dim)\n",
    "        self.b2.weight.data.zero_()\n",
    "\n",
    "    def forward(self, item_seq, user_ids, items_to_predict, for_pred=False):\n",
    "        item_embs = self.item_embeddings(item_seq)\n",
    "        user_emb = self.user_embeddings(user_ids)\n",
    "        #print(user_emb.mm(self.att_weight_user).shape)\n",
    "        #attention score\n",
    "        attention_score = torch.sigmoid(torch.matmul(item_embs, self.att_weight_item.unsqueeze(0)).squeeze() +\n",
    "                                        user_emb.mm(self.att_weight_user))\n",
    "        union_out = torch.sum(item_embs, dim=1)\n",
    "        #print(union_out.shape)\n",
    "        union_out = union_out * torch.sum(attention_score, dim=1).unsqueeze(1) \n",
    "        w2 = self.W2(items_to_predict)\n",
    "        b2 = self.b2(items_to_predict)\n",
    "         \n",
    "        if for_pred:\n",
    "            w2 = w2.squeeze()\n",
    "            b2 = b2.squeeze()\n",
    "\n",
    "            # MF\n",
    "            #print('to predict', items_to_predict.shape)\n",
    "            res = user_emb.mm(w2.t()) + b2\n",
    "            res += union_out.mm(w2.t())\n",
    "            #print('res',res.shape)\n",
    "\n",
    "        else:\n",
    "            # MF\n",
    "            res = torch.baddbmm(b2, w2, user_emb.unsqueeze(2)).squeeze()\n",
    "            res += torch.bmm(union_out.unsqueeze(1), w2.permute(0, 2, 1)).squeeze()\n",
    "            #print('to train', res.shape)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactions import Interactions\n",
    "from eval_metrics import *\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, train, test_set, topk=20):\n",
    "    num_users = train.num_users\n",
    "    num_items = train.num_items\n",
    "    batch_size = 1024\n",
    "    num_batches = int(num_users / batch_size) + 1\n",
    "    user_indexes = np.arange(num_users)\n",
    "    item_indexes = np.arange(num_items)\n",
    "    pred_list = None\n",
    "    train_matrix = train.tocsr()\n",
    "    test_sequences = train.test_sequences.sequences\n",
    "\n",
    "    for batchID in range(num_batches):\n",
    "        start = batchID * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        if batchID == num_batches - 1:\n",
    "            if start < num_users:\n",
    "                end = num_users\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        batch_user_index = user_indexes[start:end]\n",
    "\n",
    "        batch_test_sequences = test_sequences[batch_user_index]\n",
    "        batch_test_sequences = np.atleast_2d(batch_test_sequences)\n",
    "\n",
    "        batch_test_sequences = torch.from_numpy(batch_test_sequences).type(torch.LongTensor).to(device)\n",
    "        item_ids = torch.from_numpy(item_indexes).type(torch.LongTensor).to(device)\n",
    "        #print('item_ids', item_ids.shape)\n",
    "        batch_user_ids = torch.from_numpy(np.array(batch_user_index)).type(torch.LongTensor).to(device)\n",
    "\n",
    "        rating_pred = model(batch_test_sequences, batch_user_ids, item_ids, True)\n",
    "        rating_pred = rating_pred.cpu().data.numpy().copy()\n",
    "        rating_pred[train_matrix[batch_user_index].toarray() > 0] = 0\n",
    "        ind = np.argpartition(rating_pred, -topk)\n",
    "        ind = ind[:, -topk:]\n",
    "        arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n",
    "        arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
    "        batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n",
    "        #print('batch_pred_list', batch_pred_list)\n",
    "\n",
    "        if batchID == 0:\n",
    "            pred_list = batch_pred_list\n",
    "        else:\n",
    "            pred_list = np.append(pred_list, batch_pred_list, axis=0)\n",
    "\n",
    "    precision, recall, MAP, ndcg = [], [], [], []\n",
    "    for k in [5, 10, 15, 20]:\n",
    "        precision.append(precision_at_k(test_set, pred_list, k))\n",
    "        recall.append(recall_at_k(test_set, pred_list, k))\n",
    "        MAP.append(mapk(test_set, pred_list, k))\n",
    "        ndcg.append(ndcg_k(test_set, pred_list, k))\n",
    "\n",
    "    return precision, recall, MAP, ndcg\n",
    "\n",
    "\n",
    "def negsamp_vectorized_bsearch_preverif(pos_inds, n_items, n_samp=32):\n",
    "    \"\"\" Pre-verified with binary search\n",
    "    `pos_inds` is assumed to be ordered\n",
    "    reference: https://tech.hbc.com/2018-03-23-negative-sampling-in-numpy.html\n",
    "    \"\"\"\n",
    "    raw_samp = np.random.randint(0, n_items - len(pos_inds), size=n_samp)\n",
    "    pos_inds_adj = pos_inds - np.arange(len(pos_inds))\n",
    "    neg_inds = raw_samp + np.searchsorted(pos_inds_adj, raw_samp, side='right')\n",
    "    return neg_inds\n",
    "\n",
    "\n",
    "def generate_negative_samples(train_matrix, num_neg=3, num_sets=10):\n",
    "    neg_samples = []\n",
    "    for user_id, row in enumerate(train_matrix):\n",
    "        pos_ind = row.indices\n",
    "        neg_sample = negsamp_vectorized_bsearch_preverif(pos_ind, train_matrix.shape[1], num_neg * num_sets)\n",
    "        neg_samples.append(neg_sample)\n",
    "\n",
    "    return np.asarray(neg_samples).reshape(num_sets, train_matrix.shape[0], num_neg)\n",
    "\n",
    "\n",
    "def train_model(model, train_data, test_data, config):\n",
    "    num_users = train_data.num_users\n",
    "    num_items = train_data.num_items\n",
    "\n",
    "    # convert to sequences, targets and users\n",
    "    sequences_np = train_data.sequences.sequences\n",
    "    #print(sequences_np)\n",
    "    targets_np = train_data.sequences.targets\n",
    "    users_np = train_data.sequences.user_ids\n",
    "    train_matrix = train_data.tocsr()\n",
    "\n",
    "    n_train = sequences_np.shape[0]\n",
    "    logger.info(\"Total training records:{}\".format(n_train))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.l2)\n",
    "\n",
    "    record_indexes = np.arange(n_train)\n",
    "    batch_size = config.batch_size\n",
    "    num_batches = int(n_train / batch_size) + 1\n",
    "    for epoch_num in range(config.n_iter):\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        np.random.shuffle(record_indexes)\n",
    "\n",
    "        t_neg_start = time()\n",
    "        negatives_np_multi = generate_negative_samples(train_matrix, config.neg_samples, config.sets_of_neg_samples)\n",
    "        logger.info(\"Negative sampling time: {}s\".format(time() - t_neg_start))\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for batchID in range(num_batches):\n",
    "            start = batchID * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            if batchID == num_batches - 1:\n",
    "                if start < n_train:\n",
    "                    end = n_train\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            batch_record_index = record_indexes[start:end]\n",
    "\n",
    "            batch_users = users_np[batch_record_index]\n",
    "            batch_sequences = sequences_np[batch_record_index]\n",
    "            batch_targets = targets_np[batch_record_index]\n",
    "            negatives_np = negatives_np_multi[batchID % config.sets_of_neg_samples]\n",
    "            batch_neg = negatives_np[batch_users]\n",
    "\n",
    "            batch_users = torch.from_numpy(batch_users).type(torch.LongTensor).to(device)\n",
    "            batch_sequences = torch.from_numpy(batch_sequences).type(torch.LongTensor).to(device)\n",
    "            batch_targets = torch.from_numpy(batch_targets).type(torch.LongTensor).to(device)\n",
    "            batch_negatives = torch.from_numpy(batch_neg).type(torch.LongTensor).to(device)\n",
    "\n",
    "            items_to_predict = torch.cat((batch_targets, batch_negatives), 1)\n",
    "            prediction_score = model(batch_sequences, batch_users, items_to_predict, False)\n",
    "            #print(prediction_score.shape)\n",
    "            (targets_prediction, negatives_prediction) = torch.split(\n",
    "                prediction_score, [batch_targets.size(1), batch_negatives.size(1)], dim=1)\n",
    "\n",
    "            # compute the BPR loss\n",
    "            loss = -torch.log(torch.sigmoid(targets_prediction - negatives_prediction) + 1e-8)\n",
    "            loss = torch.mean(torch.sum(loss))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # clean the grad, \n",
    "            #optimizer.zero_grad()\n",
    "        epoch_loss /= num_batches\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        output_str = \"Epoch %d [%.1f s]  loss=%.4f\" % (epoch_num + 1, t2 - t1, epoch_loss)\n",
    "        logger.info(output_str)\n",
    "\n",
    "        if (epoch_num + 1) % 20 == 0:\n",
    "            model.eval()\n",
    "            precision, recall, MAP, ndcg = evaluation(model, train_data, test_data, topk=20)\n",
    "            logger.info(', '.join(str(e) for e in precision))\n",
    "            logger.info(', '.join(str(e) for e in recall))\n",
    "            logger.info(', '.join(str(e) for e in MAP))\n",
    "            logger.info(', '.join(str(e) for e in ndcg))\n",
    "            logger.info(\"Evaluation time:{}\".format(time() - t2))\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"\\n\")\n",
    "    torch.save(model.state_dict(), 'car.pkl')\n",
    "    print('model arg save complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test data        \n",
    "def split_data_sequentially(user_records, test_radio=0.2):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for item_list in user_records:\n",
    "        len_list = len(item_list)\n",
    "        num_test_samples = int(math.ceil(len_list * test_radio))\n",
    "        train_sample = []\n",
    "        test_sample = []\n",
    "        for i in range(len_list - num_test_samples, len_list):\n",
    "            test_sample.append(item_list[i])\n",
    "            \n",
    "        for place in item_list:\n",
    "            if place not in set(test_sample):\n",
    "                train_sample.append(place)\n",
    "                \n",
    "        train_set.append(train_sample)\n",
    "        test_set.append(test_sample)\n",
    "\n",
    "    return train_set, test_set\n",
    "    \n",
    "\n",
    "def generate_dataset(seq_data):\n",
    "    user_records = seq_data.tolist()\n",
    "    # split dataset\n",
    "    train_val_set, test_set = split_data_sequentially(user_records, test_radio=0.2)\n",
    "    train_set, val_set = split_data_sequentially(train_val_set, test_radio=0.1)\n",
    "\n",
    "    return train_set, val_set, train_val_set, test_set, 1083, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, train_val_set, test_set, num_users, num_items = generate_dataset(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# data arguments\n",
    "parser.add_argument('--L', type=int, default=5)\n",
    "parser.add_argument('--T', type=int, default=3)\n",
    "# train arguments\n",
    "parser.add_argument('--n_iter', type=int, default=200)\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--batch_size', type=int, default=4096)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "parser.add_argument('--l2', type=float, default=1e-3)\n",
    "parser.add_argument('--neg_samples', type=int, default=3)\n",
    "parser.add_argument('--sets_of_neg_samples', type=int, default=50)\n",
    "\n",
    "# model dependent arguments\n",
    "parser.add_argument('--d', type=int, default=50)\n",
    "config = parser.parse_args(\n",
    "    args = [\n",
    "        '--L', '5',\n",
    "        '--T', '3',\n",
    "        '--n_iter', '100',\n",
    "        '--seed', '1200',\n",
    "        '--batch_size', '500',\n",
    "        '--learning_rate', '0.001',\n",
    "        '--l2', '0.001',\n",
    "        '--neg_samples', '3',\n",
    "        '--sets_of_neg_samples', '30'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:2021-06-15 21:36:00\n",
      "INFO:__main__:Namespace(L=5, T=3, batch_size=500, d=50, l2=0.001, learning_rate=0.001, n_iter=100, neg_samples=3, seed=1200, sets_of_neg_samples=30)\n",
      "INFO:__main__:Total training records:37352\n",
      "INFO:__main__:Negative sampling time: 0.0619509220123291s\n",
      "INFO:__main__:Epoch 1 [0.4 s]  loss=881.5618\n",
      "INFO:__main__:Negative sampling time: 0.05385589599609375s\n",
      "INFO:__main__:Epoch 2 [0.3 s]  loss=665.5214\n",
      "INFO:__main__:Negative sampling time: 0.053856611251831055s\n",
      "INFO:__main__:Epoch 3 [0.3 s]  loss=600.8601\n",
      "INFO:__main__:Negative sampling time: 0.052858829498291016s\n",
      "INFO:__main__:Epoch 4 [0.3 s]  loss=557.6674\n",
      "INFO:__main__:Negative sampling time: 0.05388379096984863s\n",
      "INFO:__main__:Epoch 5 [0.3 s]  loss=524.6755\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 6 [0.3 s]  loss=502.7089\n",
      "INFO:__main__:Negative sampling time: 0.05285763740539551s\n",
      "INFO:__main__:Epoch 7 [0.3 s]  loss=484.9997\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 8 [0.3 s]  loss=468.1322\n",
      "INFO:__main__:Negative sampling time: 0.05382394790649414s\n",
      "INFO:__main__:Epoch 9 [0.3 s]  loss=457.0657\n",
      "INFO:__main__:Negative sampling time: 0.053855180740356445s\n",
      "INFO:__main__:Epoch 10 [0.3 s]  loss=443.0770\n",
      "INFO:__main__:Negative sampling time: 0.05485224723815918s\n",
      "INFO:__main__:Epoch 11 [0.3 s]  loss=433.4095\n",
      "INFO:__main__:Negative sampling time: 0.053856611251831055s\n",
      "INFO:__main__:Epoch 12 [0.3 s]  loss=423.1923\n",
      "INFO:__main__:Negative sampling time: 0.05386209487915039s\n",
      "INFO:__main__:Epoch 13 [0.3 s]  loss=410.7366\n",
      "INFO:__main__:Negative sampling time: 0.05285811424255371s\n",
      "INFO:__main__:Epoch 14 [0.3 s]  loss=402.6041\n",
      "INFO:__main__:Negative sampling time: 0.05382680892944336s\n",
      "INFO:__main__:Epoch 15 [0.3 s]  loss=393.0526\n",
      "INFO:__main__:Negative sampling time: 0.05382800102233887s\n",
      "INFO:__main__:Epoch 16 [0.3 s]  loss=386.2599\n",
      "INFO:__main__:Negative sampling time: 0.05385637283325195s\n",
      "INFO:__main__:Epoch 17 [0.3 s]  loss=377.6700\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 18 [0.3 s]  loss=373.5093\n",
      "INFO:__main__:Negative sampling time: 0.052832841873168945s\n",
      "INFO:__main__:Epoch 19 [0.3 s]  loss=366.9754\n",
      "INFO:__main__:Negative sampling time: 0.05282998085021973s\n",
      "INFO:__main__:Epoch 20 [0.3 s]  loss=360.3726\n",
      "INFO:__main__:0.3530932594644489, 0.2597414589104336, 0.19624499846106475, 0.1559556786703604\n",
      "INFO:__main__:0.14305447952191558, 0.2074657319260798, 0.2351548720902541, 0.25214155262218724\n",
      "INFO:__main__:0.2637611572791628, 0.17026402710805605, 0.12226184580726662, 0.09880942758408123\n",
      "INFO:__main__:0.3815046410812926, 0.3070509772836277, 0.25521931388142893, 0.226044050880506\n",
      "INFO:__main__:Evaluation time:0.3181726932525635\n",
      "INFO:__main__:Negative sampling time: 0.05285811424255371s\n",
      "INFO:__main__:Epoch 21 [0.3 s]  loss=356.3814\n",
      "INFO:__main__:Negative sampling time: 0.05285811424255371s\n",
      "INFO:__main__:Epoch 22 [0.3 s]  loss=350.2529\n",
      "INFO:__main__:Negative sampling time: 0.05385589599609375s\n",
      "INFO:__main__:Epoch 23 [0.3 s]  loss=346.8259\n",
      "INFO:__main__:Negative sampling time: 0.05382370948791504s\n",
      "INFO:__main__:Epoch 24 [0.3 s]  loss=342.9799\n",
      "INFO:__main__:Negative sampling time: 0.05382394790649414s\n",
      "INFO:__main__:Epoch 25 [0.3 s]  loss=341.1177\n",
      "INFO:__main__:Negative sampling time: 0.05385947227478027s\n",
      "INFO:__main__:Epoch 26 [0.3 s]  loss=333.2712\n",
      "INFO:__main__:Negative sampling time: 0.053884267807006836s\n",
      "INFO:__main__:Epoch 27 [0.3 s]  loss=323.5627\n",
      "INFO:__main__:Negative sampling time: 0.05283069610595703s\n",
      "INFO:__main__:Epoch 28 [0.3 s]  loss=325.0953\n",
      "INFO:__main__:Negative sampling time: 0.05283021926879883s\n",
      "INFO:__main__:Epoch 29 [0.3 s]  loss=319.4503\n",
      "INFO:__main__:Negative sampling time: 0.05382823944091797s\n",
      "INFO:__main__:Epoch 30 [0.3 s]  loss=313.8809\n",
      "INFO:__main__:Negative sampling time: 0.053855180740356445s\n",
      "INFO:__main__:Epoch 31 [0.3 s]  loss=309.5249\n",
      "INFO:__main__:Negative sampling time: 0.05382847785949707s\n",
      "INFO:__main__:Epoch 32 [0.3 s]  loss=307.5143\n",
      "INFO:__main__:Negative sampling time: 0.05385589599609375s\n",
      "INFO:__main__:Epoch 33 [0.3 s]  loss=304.5008\n",
      "INFO:__main__:Negative sampling time: 0.05283045768737793s\n",
      "INFO:__main__:Epoch 34 [0.3 s]  loss=302.7037\n",
      "INFO:__main__:Negative sampling time: 0.05388665199279785s\n",
      "INFO:__main__:Epoch 35 [0.3 s]  loss=291.7195\n",
      "INFO:__main__:Negative sampling time: 0.05382823944091797s\n",
      "INFO:__main__:Epoch 36 [0.3 s]  loss=290.5741\n",
      "INFO:__main__:Negative sampling time: 0.0518341064453125s\n",
      "INFO:__main__:Epoch 37 [0.3 s]  loss=288.7359\n",
      "INFO:__main__:Negative sampling time: 0.05285930633544922s\n",
      "INFO:__main__:Epoch 38 [0.3 s]  loss=282.9152\n",
      "INFO:__main__:Negative sampling time: 0.05485391616821289s\n",
      "INFO:__main__:Epoch 39 [0.3 s]  loss=282.6232\n",
      "INFO:__main__:Negative sampling time: 0.052886009216308594s\n",
      "INFO:__main__:Epoch 40 [0.3 s]  loss=283.3318\n",
      "INFO:__main__:0.24764542936287967, 0.15410895660203042, 0.11498922745460205, 0.09709141274238169\n",
      "INFO:__main__:0.09909890528036973, 0.12457951311331578, 0.14331089071486613, 0.16596493342608262\n",
      "INFO:__main__:0.18112957833179438, 0.10265301411423294, 0.07170756869495111, 0.05837357290506878\n",
      "INFO:__main__:0.2780341096957331, 0.20229373716930388, 0.16600774411352676, 0.1506576835598603\n",
      "INFO:__main__:Evaluation time:0.3041853904724121\n",
      "INFO:__main__:Negative sampling time: 0.05385470390319824s\n",
      "INFO:__main__:Epoch 41 [0.3 s]  loss=274.1658\n",
      "INFO:__main__:Negative sampling time: 0.051833152770996094s\n",
      "INFO:__main__:Epoch 42 [0.3 s]  loss=273.4223\n",
      "INFO:__main__:Negative sampling time: 0.051860809326171875s\n",
      "INFO:__main__:Epoch 43 [0.3 s]  loss=266.9278\n",
      "INFO:__main__:Negative sampling time: 0.052858591079711914s\n",
      "INFO:__main__:Epoch 44 [0.3 s]  loss=264.9729\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 45 [0.3 s]  loss=263.6414\n",
      "INFO:__main__:Negative sampling time: 0.05382823944091797s\n",
      "INFO:__main__:Epoch 46 [0.3 s]  loss=263.8367\n",
      "INFO:__main__:Negative sampling time: 0.052863359451293945s\n",
      "INFO:__main__:Epoch 47 [0.3 s]  loss=259.2438\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 48 [0.3 s]  loss=257.2863\n",
      "INFO:__main__:Negative sampling time: 0.05388617515563965s\n",
      "INFO:__main__:Epoch 49 [0.3 s]  loss=254.8675\n",
      "INFO:__main__:Negative sampling time: 0.052857398986816406s\n",
      "INFO:__main__:Epoch 50 [0.3 s]  loss=245.7602\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 51 [0.3 s]  loss=247.5169\n",
      "INFO:__main__:Negative sampling time: 0.053856611251831055s\n",
      "INFO:__main__:Epoch 52 [0.3 s]  loss=246.1744\n",
      "INFO:__main__:Negative sampling time: 0.05186128616333008s\n",
      "INFO:__main__:Epoch 53 [0.3 s]  loss=242.1587\n",
      "INFO:__main__:Negative sampling time: 0.05283045768737793s\n",
      "INFO:__main__:Epoch 54 [0.3 s]  loss=245.9800\n",
      "INFO:__main__:Negative sampling time: 0.05385541915893555s\n",
      "INFO:__main__:Epoch 55 [0.3 s]  loss=237.1606\n",
      "INFO:__main__:Negative sampling time: 0.05282998085021973s\n",
      "INFO:__main__:Epoch 56 [0.3 s]  loss=233.0138\n",
      "INFO:__main__:Negative sampling time: 0.053827762603759766s\n",
      "INFO:__main__:Epoch 57 [0.3 s]  loss=230.9689\n",
      "INFO:__main__:Negative sampling time: 0.05283331871032715s\n",
      "INFO:__main__:Epoch 58 [0.3 s]  loss=230.0575\n",
      "INFO:__main__:Negative sampling time: 0.05385637283325195s\n",
      "INFO:__main__:Epoch 59 [0.3 s]  loss=226.6023\n",
      "INFO:__main__:Negative sampling time: 0.05285787582397461s\n",
      "INFO:__main__:Epoch 60 [0.3 s]  loss=224.4952\n",
      "INFO:__main__:0.26445060018467026, 0.17774699907663777, 0.13136349646044962, 0.1095106186518925\n",
      "INFO:__main__:0.1058993367332138, 0.14406327465295773, 0.16285905247487967, 0.18643273923737722\n",
      "INFO:__main__:0.18823945829485997, 0.11207402644036994, 0.07831595758401408, 0.06381715448362683\n",
      "INFO:__main__:0.2872026444444074, 0.2191493579585668, 0.17961719627347356, 0.16262286242763893\n",
      "INFO:__main__:Evaluation time:0.3041863441467285\n",
      "INFO:__main__:Negative sampling time: 0.054854631423950195s\n",
      "INFO:__main__:Epoch 61 [0.3 s]  loss=221.6110\n",
      "INFO:__main__:Negative sampling time: 0.05388331413269043s\n",
      "INFO:__main__:Epoch 62 [0.3 s]  loss=222.0115\n",
      "INFO:__main__:Negative sampling time: 0.05385541915893555s\n",
      "INFO:__main__:Epoch 63 [0.3 s]  loss=221.0864\n",
      "INFO:__main__:Negative sampling time: 0.05385613441467285s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 64 [0.3 s]  loss=221.0240\n",
      "INFO:__main__:Negative sampling time: 0.05385541915893555s\n",
      "INFO:__main__:Epoch 65 [0.3 s]  loss=215.0292\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 66 [0.3 s]  loss=216.9201\n",
      "INFO:__main__:Negative sampling time: 0.05382394790649414s\n",
      "INFO:__main__:Epoch 67 [0.3 s]  loss=212.5184\n",
      "INFO:__main__:Negative sampling time: 0.05285763740539551s\n",
      "INFO:__main__:Epoch 68 [0.3 s]  loss=206.8478\n",
      "INFO:__main__:Negative sampling time: 0.052828311920166016s\n",
      "INFO:__main__:Epoch 69 [0.3 s]  loss=207.5565\n",
      "INFO:__main__:Negative sampling time: 0.05385541915893555s\n",
      "INFO:__main__:Epoch 70 [0.3 s]  loss=207.0263\n",
      "INFO:__main__:Negative sampling time: 0.05283093452453613s\n",
      "INFO:__main__:Epoch 71 [0.3 s]  loss=203.9979\n",
      "INFO:__main__:Negative sampling time: 0.05283021926879883s\n",
      "INFO:__main__:Epoch 72 [0.3 s]  loss=200.8496\n",
      "INFO:__main__:Negative sampling time: 0.05382800102233887s\n",
      "INFO:__main__:Epoch 73 [0.3 s]  loss=199.2951\n",
      "INFO:__main__:Negative sampling time: 0.053855180740356445s\n",
      "INFO:__main__:Epoch 74 [0.3 s]  loss=197.1267\n",
      "INFO:__main__:Negative sampling time: 0.05385637283325195s\n",
      "INFO:__main__:Epoch 75 [0.3 s]  loss=199.9879\n",
      "INFO:__main__:Negative sampling time: 0.05283069610595703s\n",
      "INFO:__main__:Epoch 76 [0.3 s]  loss=196.4762\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 77 [0.3 s]  loss=194.8390\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 78 [0.3 s]  loss=194.4924\n",
      "INFO:__main__:Negative sampling time: 0.053856611251831055s\n",
      "INFO:__main__:Epoch 79 [0.3 s]  loss=193.9824\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 80 [0.3 s]  loss=189.9660\n",
      "INFO:__main__:0.20517082179131957, 0.12779316712834654, 0.0977531548168673, 0.08591874422899308\n",
      "INFO:__main__:0.08274297462576463, 0.10321046809328654, 0.12142802994282989, 0.14845645455612902\n",
      "INFO:__main__:0.14911357340720222, 0.08349488487300122, 0.058759273205840026, 0.0483495417187354\n",
      "INFO:__main__:0.23531099334100206, 0.17084079176572967, 0.1417246071226415, 0.1309028372485362\n",
      "INFO:__main__:Evaluation time:0.3041863441467285\n",
      "INFO:__main__:Negative sampling time: 0.05385589599609375s\n",
      "INFO:__main__:Epoch 81 [0.3 s]  loss=189.3925\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 82 [0.3 s]  loss=187.2560\n",
      "INFO:__main__:Negative sampling time: 0.05283045768737793s\n",
      "INFO:__main__:Epoch 83 [0.3 s]  loss=189.1243\n",
      "INFO:__main__:Negative sampling time: 0.05382847785949707s\n",
      "INFO:__main__:Epoch 84 [0.3 s]  loss=186.7231\n",
      "INFO:__main__:Negative sampling time: 0.05283069610595703s\n",
      "INFO:__main__:Epoch 85 [0.3 s]  loss=183.5925\n",
      "INFO:__main__:Negative sampling time: 0.05382728576660156s\n",
      "INFO:__main__:Epoch 86 [0.3 s]  loss=182.3424\n",
      "INFO:__main__:Negative sampling time: 0.053855180740356445s\n",
      "INFO:__main__:Epoch 87 [0.3 s]  loss=182.3053\n",
      "INFO:__main__:Negative sampling time: 0.052829742431640625s\n",
      "INFO:__main__:Epoch 88 [0.3 s]  loss=183.0306\n",
      "INFO:__main__:Negative sampling time: 0.05282402038574219s\n",
      "INFO:__main__:Epoch 89 [0.3 s]  loss=179.7728\n",
      "INFO:__main__:Negative sampling time: 0.05382800102233887s\n",
      "INFO:__main__:Epoch 90 [0.3 s]  loss=175.9433\n",
      "INFO:__main__:Negative sampling time: 0.05285835266113281s\n",
      "INFO:__main__:Epoch 91 [0.3 s]  loss=176.7431\n",
      "INFO:__main__:Negative sampling time: 0.053856611251831055s\n",
      "INFO:__main__:Epoch 92 [0.3 s]  loss=177.3805\n",
      "INFO:__main__:Negative sampling time: 0.05385565757751465s\n",
      "INFO:__main__:Epoch 93 [0.3 s]  loss=176.7479\n",
      "INFO:__main__:Negative sampling time: 0.053854942321777344s\n",
      "INFO:__main__:Epoch 94 [0.3 s]  loss=173.0401\n",
      "INFO:__main__:Negative sampling time: 0.052831172943115234s\n",
      "INFO:__main__:Epoch 95 [0.3 s]  loss=172.9661\n",
      "INFO:__main__:Negative sampling time: 0.053855180740356445s\n",
      "INFO:__main__:Epoch 96 [0.3 s]  loss=172.8933\n",
      "INFO:__main__:Negative sampling time: 0.05382132530212402s\n",
      "INFO:__main__:Epoch 97 [0.3 s]  loss=170.2472\n",
      "INFO:__main__:Negative sampling time: 0.05283045768737793s\n",
      "INFO:__main__:Epoch 98 [0.3 s]  loss=171.4160\n",
      "INFO:__main__:Negative sampling time: 0.05385541915893555s\n",
      "INFO:__main__:Epoch 99 [0.3 s]  loss=164.2048\n",
      "INFO:__main__:Negative sampling time: 0.05283069610595703s\n",
      "INFO:__main__:Epoch 100 [0.3 s]  loss=164.9174\n",
      "INFO:__main__:0.18651892890119962, 0.12253000923360967, 0.09467528470298593, 0.08305632502308355\n",
      "INFO:__main__:0.07452211620831438, 0.09790729205032211, 0.11790038909663213, 0.14266631502348368\n",
      "INFO:__main__:0.13222530009233613, 0.07617412195205361, 0.05367528903653905, 0.04410097760967031\n",
      "INFO:__main__:0.21197854957803028, 0.15862484443513622, 0.13241776527190954, 0.12236979207279111\n",
      "INFO:__main__:Evaluation time:0.3031895160675049\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model arg save complete\n"
     ]
    }
   ],
   "source": [
    "train = Interactions(train_val_set, num_users, num_items)\n",
    "train.to_sequence(config.L, config.T)\n",
    "\n",
    "logger.info(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "logger.info(config)\n",
    "model = att_encoder(num_users, num_items, config, device).to(device)\n",
    "train_model(model, train, test_set, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Encoder Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_encoder = att_encoder(num_users, num_items, config, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_encoder.load_state_dict(torch.load('car.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = Interactions(seq_data.to_list(), num_users, num_items)\n",
    "pred_1.to_sequence(config.L, config.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_output(model, dataset):\n",
    "    num_users = dataset.num_users\n",
    "    num_items = dataset.num_items\n",
    "    batch_size = 1083\n",
    "    num_batches = int(num_users / batch_size) + 1\n",
    "    user_indexes = np.arange(num_users)\n",
    "    item_indexes = np.arange(num_items)\n",
    "    pred_list = None\n",
    "    dataset_matrix = dataset.tocsr()\n",
    "    dataset_sequences = dataset.sequences.sequences\n",
    "\n",
    "    for batchID in range(num_batches):\n",
    "        start = batchID * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        if batchID == num_batches - 1:\n",
    "            if start < num_users:\n",
    "                end = num_users\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "\n",
    "        batch_user_index = user_indexes[start:end]\n",
    "\n",
    "        batch_test_sequences = dataset_sequences[batch_user_index]\n",
    "        batch_test_sequences = np.atleast_2d(batch_test_sequences)\n",
    "\n",
    "        batch_test_sequences = torch.from_numpy(batch_test_sequences).type(torch.LongTensor).to(device)\n",
    "        item_ids = torch.from_numpy(item_indexes).type(torch.LongTensor).to(device)\n",
    "        batch_user_ids = torch.from_numpy(np.array(batch_user_index)).type(torch.LongTensor).to(device)\n",
    "\n",
    "        rating_pred = model(batch_test_sequences, batch_user_ids, item_ids, True)\n",
    "        rating_pred = rating_pred.cpu().data.numpy().copy()\n",
    "    return rating_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083, 200)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = encoder_output(best_encoder, pred_1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(output, 'clusters_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
