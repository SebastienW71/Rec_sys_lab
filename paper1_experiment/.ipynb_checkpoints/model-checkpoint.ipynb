{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionKernel(nn.Module):\n",
    "    def __init__(self, embedding_dim, device):\n",
    "        \n",
    "        super(AttentionKernel,self).__init__()\n",
    "        self.poi_embed = nn.Embedding(num_pois, embedding_dim).to(device)\n",
    "        self.linear1 = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, num_user, num_item, L, w, embedding_dim, device):\n",
    "        \n",
    "        super(model, self).__init__()\n",
    "        self.emb_dims = embedding_dim\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.L = L\n",
    "        self.w = w\n",
    "        \n",
    "        # define embedding\n",
    "        \n",
    "        self.user_embed = nn.Embedding(num_user, embedding_dim).to(device)\n",
    "        self.item_embed = nn.Embedding(num_item, embedding_dim).to(device)\n",
    "        \n",
    "        # initialize\n",
    "        self.user_embed.weight.data.normal_(0, 1.0/self.user_embed.embedding_dim)\n",
    "        self.item_embed.weight.data.normal_(0, 1.0/self.item_embed.embedding_dim)\n",
    "        \n",
    "        \n",
    "        # initialize attention kernel\n",
    "        self.att = AttentionKernel(embedding_dim, device).to(device)\n",
    "    \n",
    "    def position_embed(self, L):\n",
    "        position_embedding = np.array([[pos/np.power(1000, 2.*i)/ self.embedding_dim for i in range(self.embedding_dim)]\n",
    "                                      for pos in range(L)])\n",
    "        position_embedding[:,0::2] = np.sin(position_embedding[:,0::2])\n",
    "        position_embedding[:,1::2] = np.cos(position_embedding[:,1::2])\n",
    "        t = torch.from_numpy(position_embedding).to(device)\n",
    "        return t\n",
    "    \n",
    "    def forward(self, user_id, seq_items, target = None, for_pred = False):\n",
    "        \n",
    "        '''\n",
    "        user_id\n",
    "        seq_item = L item id that user interact before\n",
    "        target: item target\n",
    "        '''\n",
    "        \n",
    "        # sequential item embedding\n",
    "        item_embedding = self.item_embed(seq_item)\n",
    "        # item position embedding\n",
    "        position_idx = torch.range(0, self.L - 1).unsqueeze(0).expand(seq_item.size(0), -1).long()\n",
    "        position_embedding = self.item_position_embed(position_idx)\n",
    "        item_embedding_cat = item_embedding.float() + position_embedding.float()\n",
    "        \n",
    "        # attention kernel\n",
    "        attention = self.att(item_embedding_cat)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_df\n",
    "    val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Embedding(10,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36038\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.range(0,5).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
