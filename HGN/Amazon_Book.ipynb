{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing for Amazon books\n",
    "This notebook is going to transfer the raw dataset to .pickle file that is fit for HGN model.\n",
    "\n",
    "The test datasets is Amazon books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dir = 'E:\\Sebnewrepo\\Data\\hgnData/'\n",
    "rating_file = 'ratings_Books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AH2L9G3DQHHAJ</td>\n",
       "      <td>0000000116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1019865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2IIIDRK3PRRZY</td>\n",
       "      <td>0000000116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1395619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1TADCM7YWPQ8M</td>\n",
       "      <td>0000000868</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1031702400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWGH7V0BDOJKB</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1383177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3UTQPQPM4TQO0</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1374883200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id     item_id  rating   timestamp\n",
       "0   AH2L9G3DQHHAJ  0000000116     4.0  1019865600\n",
       "1  A2IIIDRK3PRRZY  0000000116     1.0  1395619200\n",
       "2  A1TADCM7YWPQ8M  0000000868     4.0  1031702400\n",
       "3   AWGH7V0BDOJKB  0000013714     4.0  1383177600\n",
       "4  A3UTQPQPM4TQO0  0000013714     5.0  1374883200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the raw csv data\n",
    "col = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(dir + rating_file, sep = ',', names = col)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Users:  8026324\n",
      "Amount of Items:  2330066\n"
     ]
    }
   ],
   "source": [
    "# count the user amount and item amount\n",
    "\n",
    "print('Amount of Users: ', len(df['user_id'].unique()))\n",
    "print('Amount of Items: ', len(df['item_id'].unique()))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Users:  7118528\n",
      "Amount of Items:  2138299\n"
     ]
    }
   ],
   "source": [
    "# delete the records with rating smaller than 4\n",
    "\n",
    "df = df[df.rating >= 4]\n",
    "print('Amount of Users: ', len(df['user_id'].unique()))\n",
    "print('Amount of Items: ', len(df['item_id'].unique())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users with < 20 interactoins are removed\n",
      "items with < 20 interactoins are removed\n",
      "num of users:76696, num of items:41265\n"
     ]
    }
   ],
   "source": [
    "# remove infrequent items and users\n",
    "from copy import deepcopy \n",
    "def rm_infrequent_items(data, min_counts):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['item_id'].value_counts()\n",
    "    df = df[df['item_id'].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"items with < {} interactoins are removed\".format(min_counts))\n",
    "    return df\n",
    "\n",
    "def rm_infrequent_users(data, min_counts):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['user_id'].value_counts()\n",
    "    df = df[df[\"user_id\"].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"users with < {} interactoins are removed\".format(min_counts))\n",
    "    return df\n",
    "\n",
    "filtered_df = rm_infrequent_users(df, 20)\n",
    "filtered_df = rm_infrequent_items(filtered_df, 20)\n",
    "print('num of users:{}, num of items:{}'.format(len(filtered_df['user_id'].unique()), len(filtered_df['item_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the item review that in itemset  \n",
    "item_list = filtered_df['item_id'].unique()\n",
    "item_set = set(item_list)\n",
    "# print(item_list[:10])\n",
    "\n",
    "review_file = 'reviews_Books_5.json.gz'\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "      \n",
    "review_dict = dict()  # [review_id] = review_text\n",
    "review_helpful = dict()\n",
    "\n",
    "# store the review into dictionary and delete the item without any review that longer than 10 words\n",
    "for l in parse(dir + review_file):\n",
    "    if l['asin'] in item_set:\n",
    "        if l['asin'] in review_dict:\n",
    "            if l['helpful'][0] / float(l['helpful'][1] + 0.01) > review_helpful[l['asin']] and len(l['reviewText']) > 10:\n",
    "                review_dict[l['asin']] = l['reviewText']\n",
    "                review_helpful[l['asin']] = l['helpful'][0] / float(l['helpful'][1] + 0.01)\n",
    "        else:\n",
    "            if len(l['reviewText']) > 10:\n",
    "                review_dict[l['asin']] = l['reviewText']\n",
    "                review_helpful[l['asin']] = l['helpful'][0] / float(l['helpful'][1] + 0.01)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is a true masterpiece in which almost every, if not every word counts.  I have read it over and over again, since a girlfriend gave me a copy just before I left for Berkeley to attend law school.  I have given copies to many friends, and recommended to others that they buy it and read it carefully.There is enormous wisdom in each chapter, especially given the materialistic and secular world in which we live - which often seems for many people to be devoid of meaning or any spiritual underpinnings.  Gibran cuts through to the beauty and essence of Life, and his words are just as profound today as when they were written in the early years of the last century.This book should be recommended reading for any young person who is old enough to wonder what Life is really all about.  Indeed, it can be picked up at any age, and it offers insights that are brilliant and beautifully written.I urge anyone, who is thinking about buying it, to do so.  Just peruse a copy at a local bookstore or library, and you are apt to become enthralled.  It may take several readings to soak up its essence; and even after many years, I still find nuances and learn more each time I reread it.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict['000100039X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980039920159681"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_helpful['000100039X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sequential data per user\n",
    "def convert_data(data):\n",
    "    df = deepcopy(data)\n",
    "    df_ordered = df.sort_values(['timestamp'], ascending=True)\n",
    "    data = df_ordered.groupby('user_id')['item_id'].apply(list)\n",
    "    unique_data = df_ordered.groupby('user_id')['item_id'].nunique()\n",
    "    # delete users whose items are less than 10\n",
    "    data = data[unique_data[unique_data >= 10].index]\n",
    "    print(data[:10])\n",
    "    print(len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "A002359833QJM7OQHCXWY    [B00BUKRALG, B00BWY3UKU, B004OEKH7Y, 076420477...\n",
      "A00463782V7TKAP9EMNL     [B00ES4C28C, 1941450008, B004XJ6922, 148481477...\n",
      "A00579222Q4YKY0J53RLA    [193415766X, 0345492641, 1451608160, 193639924...\n",
      "A006458827ALF2J23JJTO    [1489539042, 1482616319, B00DBE8QDU, B00DUFCJ1...\n",
      "A0092581WFYQNV4KMUZ3     [0425263916, 0615744257, 0060734019, 032147404...\n",
      "A0099735VDZ3HDCAAYKL     [0451228219, 0451229444, B008XOWVVG, B006BFX4U...\n",
      "A010971113OD625HDB6X8    [0606262520, 0985023058, 0373210515, 148270683...\n",
      "A010997525FU27TAPMJCG    [0451165209, 1442346272, 0470101474, 039331929...\n",
      "A01628721NLXK7ENDWDC9    [B00558WOZG, 1425746845, B004WLOGYE, B004Z1N2G...\n",
      "A01631062UX24GI4LJKF     [B00EOUWEW4, B00D6IAJHM, B00FXSDNX0, B00G002LH...\n",
      "Name: item_id, dtype: object\n",
      "52406\n"
     ]
    }
   ],
   "source": [
    "seq_data = convert_data(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52406 41264\n"
     ]
    }
   ],
   "source": [
    "# user sequential data to dict\n",
    "user_item_dict = seq_data.to_dict()\n",
    "user_mapping = []\n",
    "item_set = set()\n",
    "\n",
    "# create user and item mapping table\n",
    "for user_id, item_list in seq_data.iteritems():\n",
    "    user_mapping.append(user_id)\n",
    "    for item_id in item_list:\n",
    "        item_set.add(item_id)\n",
    "item_mapping = list(item_set)\n",
    "\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30760, 24286, 31426, 25106, 25128, 34254, 35686, 24214, 30357, 16476, 11606], [37352, 32642, 8719, 27827, 10324, 8990, 6884, 24739, 21241, 21013, 12179, 16461, 25351, 12894, 20743, 26668, 28915, 15250, 5754, 10682], [26150, 30666, 27835, 23353, 39608, 3783, 33181, 10604, 35354, 14855, 7776, 13858, 9705], [14876, 985, 11882, 29715, 19554, 34565, 34633, 33480, 15273, 41192, 36058, 56, 16117, 11583, 15022, 19315, 27686, 30378, 27991, 38207, 16380, 13595, 3976, 35546, 24128, 24605, 11486, 26977, 2593, 40227, 32990, 18319, 29179, 36062, 3029, 24897, 32574, 28105], [30660, 11463, 30343, 17479, 20555, 22125, 15747, 23059, 38904, 19705, 23941, 21548, 26676, 34847, 31675, 32303, 21446, 18125, 40249]]\n"
     ]
    }
   ],
   "source": [
    "# create mapping index\n",
    "def generate_inverse_mapping(data_list):\n",
    "    inverse_mapping = dict()\n",
    "    for inner_id, true_id in enumerate(data_list):\n",
    "        inverse_mapping[true_id] = inner_id\n",
    "    return inverse_mapping\n",
    "\n",
    "def convert_to_inner_index(user_records, user_mapping, item_mapping):\n",
    "    inner_user_records = []\n",
    "    user_inverse_mapping = generate_inverse_mapping(user_mapping)\n",
    "    item_inverse_mapping = generate_inverse_mapping(item_mapping)\n",
    "\n",
    "    for user_id in range(len(user_mapping)):\n",
    "        real_user_id = user_mapping[user_id]\n",
    "        item_list = list(user_records[real_user_id])\n",
    "        for index, real_item_id in enumerate(item_list):\n",
    "            item_list[index] = item_inverse_mapping[real_item_id]\n",
    "        inner_user_records.append(item_list)\n",
    "\n",
    "    return inner_user_records, user_inverse_mapping, item_inverse_mapping\n",
    "\n",
    "inner_data_records, user_inverse_mapping, item_inverse_mapping = convert_to_inner_index(user_item_dict, user_mapping, item_mapping)\n",
    "print(inner_data_records[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(inner_data_records, 'Books_item_sequences')\n",
    "save_obj(user_mapping, 'Books_user_mapping')\n",
    "save_obj(item_mapping, 'Books_item_mapping') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
